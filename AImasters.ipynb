{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as np\n",
    "import cupyx.scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    'G:\\My Drive\\ML\\\\ai-masters-ds-contest-2022\\\\train2022.csv')\n",
    "test = pd.read_csv('G:\\My Drive\\ML\\\\ai-masters-ds-contest-2022\\\\test2022.csv')\n",
    "train.set_index('id', inplace=True)\n",
    "test.set_index('id', inplace=True)\n",
    "X = train.iloc[:, :402]\n",
    "y = train.iloc[:, 402:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def row2pics(X_r, y_r, test_vals=False):\n",
    "    a1 = np.array(X_r[2:]).reshape((20, 20))\n",
    "    a2 = np.array(y_r).reshape((20, 20))\n",
    "    plt.imshow(a1, interpolation='nearest')\n",
    "    plt.show()\n",
    "    plt.imshow(a2, interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.scipy.signal import convolve2d\n",
    "\n",
    "'---------------------CONSTANTS---------------------'\n",
    "n = 20\n",
    "alpha = np.tile(np.floor(np.arange(n) / (n / 3)) + 1, (n, 1))\n",
    "beta = alpha.T + 1\n",
    "alpha, beta = np.minimum(alpha, beta), np.maximum(alpha, beta)\n",
    "gamma = (alpha + beta) / 2\n",
    "'---------------------------------------------------'\n",
    "\n",
    "\n",
    "def make_step(X):\n",
    "    nbrs_count = convolve2d(X, np.ones(\n",
    "        (3, 3)), mode='same', boundary='wrap') - X\n",
    "    return ((nbrs_count <= beta) & (nbrs_count >= gamma)) | (X & (nbrs_count >= alpha) & (nbrs_count <= gamma))\n",
    "\n",
    "\n",
    "def makeksteps(X, k=3):\n",
    "    for i in range(k):\n",
    "        X = make_step(X)\n",
    "    return (X)\n",
    "\n",
    "\n",
    "def init_pole(n):\n",
    "    return np.random.randint(2, size=(n, n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(k:int=1, n_lines:int = 50000):\n",
    "    # порождение обучающей выборки\n",
    "    n = 20  # число строк и столбцов на поле\n",
    "    X = []\n",
    "    y = []\n",
    "    for t in np.arange(n_lines):\n",
    "        k = 1\n",
    "        X0 = init_pole(n=n)  # инициализация (КОД НЕ ДАН)\n",
    "        X1 = makeksteps(X0.copy(), k=k)  # сделать k шагов\n",
    "        X.append(np.array(X1))\n",
    "        y.append(np.array(X0))\n",
    "    X = np.array(X).reshape((-1, 1, 20, 20))\n",
    "    y = np.array(y).reshape((-1, 1, 20, 20))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main sequel nn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.scipy import signal\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers: list, loss_func: str) -> None:\n",
    "        global mse, mse_prime, binary_crossentropy, binary_crossentropy_prime\n",
    "        self.network = layers\n",
    "        self.loss_func = {\n",
    "            'mse': mse,\n",
    "            'binary_crossentropy': binary_crossentropy\n",
    "        }[loss_func]\n",
    "\n",
    "        self.loss_func_prime = {\n",
    "            'mse': mse_prime,\n",
    "            'binary_crossentropy': binary_crossentropy_prime\n",
    "        }[loss_func]\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, learning_rate: float, n_epochs: int, OD_iter: int = 10, delta=0.0001, opimizer: str = 'adam') -> None:\n",
    "        best_model = None\n",
    "        min_error = np.inf\n",
    "        counter = 0\n",
    "        for e in np.arange(n_epochs):\n",
    "            error = 0\n",
    "            for x, y in zip(X, Y):\n",
    "                output = x\n",
    "                for layer in self.network:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                error += self.loss_func(y, output)\n",
    "                grad = self.loss_func_prime(y, output)\n",
    "\n",
    "                for layer in self.network[::-1]:\n",
    "                    grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            error /= len(X)\n",
    "            counter += 1\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_model = self.network.copy()\n",
    "                if error + delta < min_error:\n",
    "                    counter = 0\n",
    "\n",
    "            if OD_iter:\n",
    "                if counter == OD_iter:\n",
    "                    break\n",
    "\n",
    "            print(f\"Epoch {e+1}/{n_epochs}: error = {error}\")\n",
    "\n",
    "        self.network = best_model\n",
    "\n",
    "    def predict_r(self, X_r: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.network:\n",
    "            X_r = layer.forward(X_r)\n",
    "        return X_r\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.array([np.array(self.predict_r(X[i])) for i in np.arange(X.shape[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base classes + optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, weights, learning_rate: float) -> None:\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def backward_pass(self, gradients: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float, optimizer: Optimizer = None) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, weights, learning_rate: float) -> None:\n",
    "        self.weights = weights\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def backward_pass(self, gradients: np.ndarray) -> np.ndarray:\n",
    "        self.weights -= self.learning_rate * gradients\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "        self.theta = weights\n",
    "\n",
    "    def backward_pass(self, gradient):\n",
    "        self.t = self.t + 1\n",
    "        self.m = self.beta1*self.m + (1 - self.beta1)*gradient\n",
    "        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)\n",
    "        m_hat = self.m/(1 - self.beta1**self.t)\n",
    "        v_hat = self.v/(1 - self.beta2**self.t)\n",
    "        self.theta -= self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))\n",
    "        return self.theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, optimizer: 'str' = 'adam') -> None:\n",
    "        self.optimizer = {'adam': Adam,\n",
    "                          'sgd': SGD\n",
    "                          }[optimizer]\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.w_optimizer = self.optimizer(self.weights)\n",
    "\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "        self.b_optimizer = self.optimizer(self.bias)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        weidhts_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights = self.w_optimizer.backward_pass(weidhts_gradient)\n",
    "        self.bias = self.b_optimizer.backward_pass(output_gradient)\n",
    "        return input_gradient\n",
    "\n",
    "\n",
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size: int, depth: int, padding: bool = True, optimizer: str = 'adam') -> None:\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth = depth\n",
    "        self.output_shape = (depth, input_height, input_width) if padding else (\n",
    "            depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "        self.optimizer = {'adam': Adam,\n",
    "                          'sgd': SGD\n",
    "                          }[optimizer]\n",
    "        self.kernels_optimizer = self.optimizer(self.kernels)\n",
    "        self.biases_optimizer = self.optimizer(self.biases)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                # self.output[i] += signal.convolve2d(self.input[j], self.kernels[i, j], mode='valid') [j] removed bcs only 1 input\n",
    "                self.output[i] += signal.correlate2d(\n",
    "                    self.input[j], self.kernels[i, j], mode='same')\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                # kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], mode='valid') [j] removed bcs only 1 input\n",
    "                kernels_gradient[i, j] = signal.correlate2d(\n",
    "                    self.input[j], output_gradient[i], mode='valid')\n",
    "                input_gradient[j] += signal.convolve2d(\n",
    "                    output_gradient[i], self.kernels[i, j], mode='same')\n",
    "\n",
    "        self.kernels = self.kernels_optimizer.backward_pass(kernels_gradient)\n",
    "        self.biases = self.biases_optimizer.backward_pass(output_gradient)\n",
    "        return input_gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        return np.reshape(output_gradient, self.input_shape)\n",
    "\n",
    "\n",
    "class MaxPool2d(Layer):\n",
    "    def __init__(self, input_shape, kernel_size: int, padding: bool = True) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_shape = input_shape if padding else (\n",
    "            input_shape[0], input_shape[1] - kernel_size + 1, input_shape[2] - kernel_size + 1)\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        for i in np.arange(self.input_shape[0]):\n",
    "            for j in np.arange(self.input_shape[1]):\n",
    "                for k in np.arange(self.input_shape[2]):\n",
    "                    self.output[i, j//self.kernel_size, k//self.kernel_size] = np.max(\n",
    "                        self.input[i, j:j+self.kernel_size, k:k+self.kernel_size])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "        for i in range(self.input_shape[0]):\n",
    "            for j in range(self.input_shape[1]):\n",
    "                for k in range(self.input_shape[2]):\n",
    "                    input_gradient[i, j:j+self.kernel_size, k:k+self.kernel_size] += (self.input[i, j//self.kernel_size, k//self.kernel_size]\n",
    "                                                                                      == self.input[i, j:j+self.kernel_size, k:k+self.kernel_size])*output_gradient[i, j//self.kernel_size, k//self.kernel_size]\n",
    "        return input_gradient\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, input_shape, epsilon=1e-8):\n",
    "        self.input_shape = input_shape\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones(input_shape)\n",
    "        self.beta = np.zeros(input_shape)\n",
    "        self.gamma_optimizer = Adam(self.gamma)\n",
    "        self.beta_optimizer = Adam(self.beta)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input\n",
    "        self.mean = np.mean(input, axis=0)\n",
    "        self.std = np.std(input, axis=0)\n",
    "        self.output = (input - self.mean) / (self.std + self.epsilon)\n",
    "        return self.output * self.gamma + self.beta\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate:float) -> np.ndarray:\n",
    "        self.gamma = self.gamma_optimizer.backward_pass(output_gradient)\n",
    "        self.beta = self.beta_optimizer.backward_pass(output_gradient)\n",
    "        return (output_gradient * self.gamma) / (self.std + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true)/np.size(y_true)\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return np.mean(np.maximum(y_pred, 0) - y_pred * y_true + np.log(1 + np.exp(-np.abs(y_pred))))\n",
    "\n",
    "\n",
    "def binary_crossentropy_prime(y_true, y_pred):\n",
    "    return (-(y_true / y_pred) + ((1 - y_true) / (1 - y_pred)))/np.size(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime) -> None:\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self) -> None:\n",
    "        def tanh(x): return np.tanh(x)\n",
    "        def tanh_prime(x): return 1 - np.tanh(x) ** 2\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self) -> None:\n",
    "        def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "        def sigmoid_prime(x): return sigmoid(x) * (1 - sigmoid(x))\n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self) -> None:\n",
    "        def relu(x): return np.maximum(0, x)\n",
    "        def relu_prime(x): return (x > 0).astype(float)\n",
    "        super().__init__(relu, relu_prime)\n",
    "\n",
    "\n",
    "class LeakyReLU(Activation):\n",
    "    def __init__(self) -> None:\n",
    "        def leaky_relu(x): return np.maximum(0.01 * x, x)\n",
    "        def leaky_relu_prime(x): return 1.0 if x > 0.0 else 0.01\n",
    "        leaky_relu_prime = np.vectorize(leaky_relu_prime)\n",
    "        super().__init__(leaky_relu, leaky_relu_prime)\n",
    "        super().__init__(leaky_relu, leaky_relu_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CUDA Device 0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cuda.Device(0).use() # Set the GPU to use (RTX 2060m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения - CNN в 2 вариантах (микшировал слои выше для sequal нейронки). Обучаем модели смотреть на шаг назад (генерим свои данные по данному образцу). На каждое количество шагов лучше использовать отдельную аналогичную нейронку, т.к. при повторном использовании одной модели ошибка на отдельном поле увеличивается с каждым шагом.\n",
    "\n",
    "Свёрточные слои склеиваю batch normalization для общего импрувмента и LeakyRelu в качестве активационной функции. На конце сигмоид т.к. нужен вывод вероятности от 0 до 1, жива ли клетка поля. loss func - mse, т.к. binary crossentropy оказалась нестабильной (даже не смотря на её улучшенную версию, кою подглядел у pytorch)\n",
    "\n",
    "В обоих случаях задействован adam оптимайзер, т.к., судя по статьям и общему опыту комьюнити, это 1 из лучших вариантов. \n",
    "\n",
    "В model3 заместо двух свёрточных слоёв используются обычные нейроны\n",
    "\n",
    "Для обучения используются только numpy и scipy (их cupy версии для задействования gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: error = 0.4781859898580645\n",
      "Epoch 2/1000: error = 0.4684473940275236\n",
      "Epoch 3/1000: error = 0.46609941598441773\n",
      "Epoch 4/1000: error = 0.4403620032883539\n",
      "Epoch 5/1000: error = 0.37174852528836394\n",
      "Epoch 6/1000: error = 0.26226437611798226\n",
      "Epoch 7/1000: error = 0.26102450419918627\n",
      "Epoch 8/1000: error = 0.2603947483814405\n",
      "Epoch 9/1000: error = 0.3615033868173209\n"
     ]
    }
   ],
   "source": [
    "model3 = NN([\n",
    "    Convolutional((1, 20, 20), kernel_size=5, depth=1, padding=True),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization((1, 20, 20)),\n",
    "    Reshape((1, 20, 20), (400,1)),\n",
    "    \n",
    "    Dense(400, 800),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization((800, 1)),\n",
    "    Dense(800, 400),\n",
    "    Reshape((400,1), (20, 20)),\n",
    "    Sigmoid()\n",
    "\n",
    "\n",
    "], loss_func='mse')\n",
    "model3.fit(X_train, y_train, n_epochs=1000, learning_rate=0.1, OD_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: error = 0.2789834721333651\n",
      "Epoch 2/1000: error = 0.25138456139426757\n",
      "Epoch 3/1000: error = 0.2491737707802211\n",
      "Epoch 4/1000: error = 0.24850790139201964\n",
      "Epoch 5/1000: error = 0.2486578783235552\n",
      "Epoch 6/1000: error = 0.24849978804541978\n",
      "Epoch 7/1000: error = 0.24857417241287275\n",
      "Epoch 8/1000: error = 0.24872210398244343\n",
      "Epoch 9/1000: error = 0.24902836379247772\n"
     ]
    }
   ],
   "source": [
    "model4 = NN([\n",
    "    Convolutional((1, 20, 20), kernel_size=5, depth=4, padding=True),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization((4, 20, 20)),\n",
    "    Convolutional((4, 20, 20), kernel_size=5, depth=4, padding=True),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization((4, 20, 20)),\n",
    "\n",
    "    Convolutional((4, 20, 20), kernel_size=5, depth=1, padding=True),\n",
    "    Sigmoid()\n",
    "\n",
    "], loss_func='mse')\n",
    "model4.fit(X_train, y_train, n_epochs=1000, learning_rate=0.1, OD_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: error = 0.40332036923221154\n",
      "Epoch 2/1000: error = 0.31640590199607127\n",
      "Epoch 3/1000: error = 0.28528640615610107\n",
      "Epoch 4/1000: error = 0.2840457466478288\n",
      "Epoch 5/1000: error = 0.27294998655715436\n",
      "Epoch 6/1000: error = 0.25321410873429495\n",
      "Epoch 7/1000: error = 0.2472476280012612\n",
      "Epoch 8/1000: error = 0.24663503377930956\n",
      "Epoch 9/1000: error = 0.24631448991732227\n",
      "Epoch 1/1000: error = 0.4770091727841832\n",
      "Epoch 2/1000: error = 0.43422753064228176\n",
      "Epoch 3/1000: error = 0.4249286029130695\n",
      "Epoch 4/1000: error = 0.42054326557459104\n",
      "Epoch 5/1000: error = 0.4392488404603194\n",
      "Epoch 6/1000: error = 0.4500645196900783\n",
      "Epoch 7/1000: error = 0.43850816164515827\n",
      "Epoch 8/1000: error = 0.4366221794640462\n",
      "Epoch 9/1000: error = 0.4296046475993485\n",
      "Epoch 1/1000: error = 0.41980640348486764\n",
      "Epoch 2/1000: error = 0.4267258008198453\n",
      "Epoch 3/1000: error = 0.43171171443088446\n",
      "Epoch 4/1000: error = 0.43839332534342595\n",
      "Epoch 5/1000: error = 0.4400362884109987\n",
      "Epoch 6/1000: error = 0.4414597221258722\n",
      "Epoch 7/1000: error = 0.44108635665090673\n",
      "Epoch 8/1000: error = 0.44237346532460425\n",
      "Epoch 9/1000: error = 0.4414227614887252\n",
      "Epoch 1/1000: error = 0.48157878419307293\n",
      "Epoch 2/1000: error = 0.471322758815154\n",
      "Epoch 3/1000: error = 0.4654970517457286\n",
      "Epoch 4/1000: error = 0.4615194326254103\n",
      "Epoch 5/1000: error = 0.4605811077767975\n",
      "Epoch 6/1000: error = 0.4641638790001547\n",
      "Epoch 7/1000: error = 0.4714957743506088\n",
      "Epoch 8/1000: error = 0.47029566765236436\n",
      "Epoch 9/1000: error = 0.4699975489109446\n",
      "Epoch 1/1000: error = 0.42273515518187676\n",
      "Epoch 2/1000: error = 0.39457005132191014\n",
      "Epoch 3/1000: error = 0.3743698152143786\n",
      "Epoch 4/1000: error = 0.3806680897813803\n",
      "Epoch 5/1000: error = 0.38337810413886053\n",
      "Epoch 6/1000: error = 0.37222716997268146\n",
      "Epoch 7/1000: error = 0.37956033454482824\n",
      "Epoch 8/1000: error = 0.3746218597540707\n",
      "Epoch 9/1000: error = 0.3604487623879234\n",
      "Epoch 1/1000: error = 0.47917578577266207\n",
      "Epoch 2/1000: error = 0.4800420532606286\n",
      "Epoch 3/1000: error = 0.4690797193143444\n",
      "Epoch 4/1000: error = 0.46207179585480046\n",
      "Epoch 5/1000: error = 0.4603619768320147\n",
      "Epoch 6/1000: error = 0.46495285118890683\n",
      "Epoch 7/1000: error = 0.4551029877544361\n",
      "Epoch 8/1000: error = 0.4627603942810652\n",
      "Epoch 9/1000: error = 0.4779724768876922\n",
      "Epoch 1/1000: error = 0.3028213062453835\n",
      "Epoch 2/1000: error = 0.27052850993140376\n",
      "Epoch 3/1000: error = 0.2634082801792946\n",
      "Epoch 4/1000: error = 0.2560211733891383\n",
      "Epoch 5/1000: error = 0.25314545464741445\n",
      "Epoch 6/1000: error = 0.2510784261628175\n",
      "Epoch 7/1000: error = 0.24990055800688551\n",
      "Epoch 8/1000: error = 0.24948074575321028\n",
      "Epoch 9/1000: error = 0.24897715746587956\n",
      "Epoch 1/1000: error = 0.4854108481759077\n",
      "Epoch 2/1000: error = 0.44896026861436295\n",
      "Epoch 3/1000: error = 0.3954675203352687\n",
      "Epoch 4/1000: error = 0.39360103201125335\n",
      "Epoch 5/1000: error = 0.40539850429658475\n",
      "Epoch 6/1000: error = 0.4044836132478166\n",
      "Epoch 7/1000: error = 0.43226015440078225\n",
      "Epoch 8/1000: error = 0.44005835832280327\n",
      "Epoch 9/1000: error = 0.4270557006442517\n"
     ]
    }
   ],
   "source": [
    "models4 = [model4]\n",
    "models3 = [model3]\n",
    "\n",
    "for i in np.arange(2, 6):\n",
    "    X_train_k, y_train_k = generate_data(i)\n",
    "    model3_k = NN([\n",
    "        Convolutional((1, 20, 20), kernel_size=5, depth=1, padding=True),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization((1, 20, 20)),\n",
    "        Reshape((1, 20, 20), (400, 1)),\n",
    "\n",
    "        Dense(400, 800),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization((800, 1)),\n",
    "        Dense(800, 400),\n",
    "        Reshape((400, 1), (20, 20)),\n",
    "        Sigmoid()\n",
    "    ], loss_func='mse')\n",
    "    \n",
    "    model4_k = NN([\n",
    "        Convolutional((1, 20, 20), kernel_size=5, depth=4, padding=True),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization((4, 20, 20)),\n",
    "        Convolutional((4, 20, 20), kernel_size=5, depth=4, padding=True),\n",
    "        LeakyReLU(),\n",
    "        BatchNormalization((4, 20, 20)),\n",
    "\n",
    "        Convolutional((4, 20, 20), kernel_size=5, depth=1, padding=True),\n",
    "        Sigmoid()\n",
    "\n",
    "    ], loss_func='mse')\n",
    "    \n",
    "    model4_k.fit(X_train_k, y_train_k, n_epochs=1000, learning_rate=0.1, OD_iter=10)\n",
    "    models4.append(model4_k)\n",
    "    \n",
    "    model3_k.fit(X_train_k, y_train_k, n_epochs=1000, learning_rate=0.1, OD_iter=10)\n",
    "    models3.append(model3_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(models: list[NN], X_test= test) -> None:\n",
    "    submission = pd.DataFrame()\n",
    "    for i in range(1, 6):\n",
    "        X_k = test.drop('regime', axis=1)\n",
    "        indexes = X_k[X_k['steps']==i].index\n",
    "        y_pred = pd.DataFrame()\n",
    "        y_pred['id'] = indexes\n",
    "        X_k = np.array(X_k[X_k['steps']==i].drop('steps', axis=1).to_numpy()).reshape((-1, 1, 20, 20))\n",
    "        sos = pd.DataFrame([np.asnumpy(models[i-1].predict_r(X_k[j]).reshape((400))) for j in np.arange(X_k.shape[0])], columns=[f'y_{i}' for i in range(400)])\n",
    "        \n",
    "        y_pred =  pd.concat([y_pred, sos], axis=1)\n",
    "        submission = pd.concat([submission, y_pred], axis=0)\n",
    "    \n",
    "    return submission\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = submit(models3, test)\n",
    "sub2 = submit(models4, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('submission_1.csv', index=False)\n",
    "sub2.to_csv('submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "                9,\n",
       "            ...\n",
       "            49990, 49991, 49992, 49993, 49994, 49995, 49996, 49997, 49998,\n",
       "            49999],\n",
       "           dtype='int64', name='id', length=50000)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8ea2be8e3a80cdd0813e192120f87fec1980ce711e95cb955e3a82156923bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
